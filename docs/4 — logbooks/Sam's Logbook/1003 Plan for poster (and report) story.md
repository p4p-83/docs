![](Image%203%201.jpeg)

I've essentially accepted that it's going to be too difficult to convey absolutely every aspect of our project in the poster and report — there's too much, and some of it is too disparate. Just like for the seminar, I'm going to have to be a little selective, but the seminar structure won't work here: we're no longer doing CV, and we've got more time and space to discuss everything. A new structure is in order, and that's what I've tried to jot down above.

Essentially, I propose framing the problem statement as being specifically to provide a mechanised PCB-assembly method to an operator without sacrificing the visual clarity of the top-down vantage point they'd usually have when assembling with tweezers. Obviously we arrived at this problem statement through exploration of the problem space and so forth, but that isn't really our research — we could just as easily have been handed that objective and started from there. And that's exactly how I'll treat it in presentation — we just take that for granted and move through the engineering method and findings. I can then loop back to the bigger picture as necessary later on.

Anyway, the rest is in the diagram.

## Alternative approaches

I do mention "alternative approaches" in the diagram. These were other ways of achieving substantially the same effect (i.e. competing solutions to the problem statement) that we either discounted or didn't have time to pursue.

(I think one of our bigger contributions to the research is actually in boiling down the "rapid prototyping PnP" to this one engineering problem, and now being able to describe it as such with some degree of clarity. That's really what we've been fighting subconsciously all year, and now that we can frame it that way, more rigorous and numbers-based engineering analyses can follow on, perhaps even comparing and contrasting these alternative approaches.)

1. The approach we went through with — an extended (and, in our case, folded) *z* action on the head that allows a camera to be switched in underneath the component to capture a real-time "upwards" video feed.
2. A variation on the option that we went through with, where the mechanically-obtained upward camera feed is replaced with a software-simulated upward feed. This would be achieved by taking a single still image from camera fixed to the machine floor once after component pickup, and then just digitally manipulating it to track nozzle rotations etc. The downward feed will still be live video and so the backend would still produce the composted live video for MediaMTX dispatch to the same front-end. The more sophisticated compositing would likely introduce a need to do lens distortion correction on the live video feed.
3. If still images work for the upward feed (i.e. the machine is repeatable enough to manage this) then acquiring still images from the downward feed could also be considered. In such a setup, all of the work would be done at the front end based on cached images from both cameras. A "map" could be maintained and the compositing would be derived from these static image tiles, removing the need for a live video stream entirely and eliminating a source of latency. The machine itself would just work asynchronously to execute an instruction queue as sent from the front end, consisting of pick and place operations and requests for new upward and downward still images to keep the front-end image tile cache up-to-date. Such a method becomes a predominantly software based project. The investment required to get anything working is a step beyond method (2) above, but the extra initial investment would pay off by providing a very low-latency and extensible platform on which to develop some extremely real-time, feed-forward placement software to really push the UI/flow envelope.
4. Back to doing it in hardware: the above three methods trade off varying degrees of mechanical and software complexity to achieve the initial project aim of a zero-parallax visual perspective through compositing at least two images together. However, it would be possible to take inspiration from biology and current practice and try to just put a single camera directly above the component being placed, bypassing compositing altogether. (This does mean that you end up with a top-down view of the component being placed, rather than the bottom-up, underside view of (1), (2) and (3), for better or worse.) Engineers currently achieve the pure top-down view by holding the component with tweezers, where the tweezers are held at an oblique angle to avoid obstructing the view. You could achieve this in machine form either by strapping a pair of tweezers to the machine (the really bio-inspired solution; essentially making the project engineering brief "develop a mechatronic pair of tweezers with overhead microscope") or by fabricating a right-angle vacuum nozzle. (A right angle nozzle would provide varying degrees of obstruction, largely depending on manufacturing capabilities: for example, if you put two microscope slides together with a small air gap between them, and sealed this aside from a small hole in the lower sheet near the centre and a feed hole on one slide, you'd essentially have a transparent, pancake-style nozzle made of planar glass surfaces. It'd be essentially invisible, but it would be a nightmare from a clearance perspective. Another idea just uses a thin, bent plastic tube that acts like a normal nozzle for a bit but then bends out of the camera's way, but rotation then becomes more challenging and you occlude part of the component.)
5. A variation on (4) where your nozzle is also your lens, or you otherwise do some optics design to get a view around the nozzle. I'm not an optics person but that sounds hard and expensive.
6. You achieve the same effect as (5) but with stereoscopic vision / a plurality of cameras. If you have image data for each part of the scene from at least one camera, you may be able to reconstruct a single frame computationally. This was Nitish's original suggestion, or at least pretty close to it — he was discussing dealing with such occlusions, albeit perhaps more so from a "perspective correction" point of view.

I'm sure there are more. There will always be other ideas, but these are six obvious and not-so-obvious options to satisfy our initial brief.

We ruled out (3) and (6) as they didn't align with our skillset or interests — neither of us are software engineering majors. (4) was out because we're not mechatronics people and this wasn't the "existing machine technology" we had in mind (this one is a bit tangential to the *intent* of the brief, if not the wording itself). (5) is similarly tangential to the brief and way of piste — and definitely not in budget. Similar budget constraints impaired our confidence to try (2) — we were working with a janky pen plotter, not a LumenPnP.

So in the end, we did (1). If I did it again, I'd only do it using method (2). (1) might be able to work, some day, but it's too far from optimal. It cannot compete with (2), particularly given that our present software is already pretty compatible with that option. Also, having (2) work would just be a much more practical end result anyway.